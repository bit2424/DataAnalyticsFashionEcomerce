# -*- coding: utf-8 -*-
"""Price_Predictions_in_EcommerceWish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RsVZXe6zZZSgjP4xr7WVrGcjoDp_zkMj

# Predicciones de precio en wish

En este nootebook se utiliza un dataset de las ventas de verano en el 2019 de Wish, la intención es ser capaces de predecir el precio de un producto basados en caracteristicas del producto que podamos observar o determinar, y qué estén incluidos en el dataset.

El primer paso es importar todas las librerias necesarias para el procesamiento de los datos e importar el dataset montado en una carpeta de drive.
"""
import numpy as np
import pandas as pd
import seaborn as sns
from joblib import dump
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import KFold, cross_val_score
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
# models
from sklearn.tree import DecisionTreeClassifier
from xgboost.sklearn import XGBClassifier

from common import Commons
from common import pcolors


# !pip install pywaffle --quiet
# from wordcloud import WordCloud

# from google.colab import drive

# drive.mount('/content/drive')

class PriceModel:
    def __init__(self):
        """
        Inicializa un nuevo PriceModel, cargando el dataset del archivo csv en un dataframe de pandas.
        Se definen dos atributos:
            * df -> El dataset principal
            * df_interesting -> Las caracteristicas de interes del dataframe.
        """
        # df = pd.read_csv("/content/drive/MyDrive/Proyecto Final IA/summer-products-with-rating-and-performance_2020-08.csv")
        self.df = pd.read_csv("./Resources/Datasets/summer-products.csv")
        # /summer-products-with-rating-and-performance_2020-08.csv
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- ORIGINAL ----------------------------\n" + pcolors.ENDC + pcolors.ENDC)
        print(self.df.columns)
        print("\n")
        print(self.df.shape)
        # https://www.kaggle.com/jmmvutu/summer-products-and-sales-in-ecommerce-wish
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- ORIGINAL ---------------------------->>>\n\n" + pcolors.ENDC)
        self.df_interesting = Commons.extract_interesting_features(self.df, "price")

    def execute(self):
        """
        Ejecuta las tareas de analisis y limpieza de datos y posterior a ello evalua y entrena los mejores modelos
        """
        Commons.plot_missing_data(self.df_interesting)
        Commons.clean(self.df_interesting)
        """Para ver la calidad de las variables que escogimos decidimos ver el indice de correlación que tienen respecto al precio de venta."""
        Commons.print_correlation_map(self.df_interesting, 'price')
        Commons.plot_colors_vs_feature(self.df_interesting, 'price')
        self.df_interesting = Commons.one_hot_encode(self.df_interesting)
        # correlation again
        Commons.print_correlation_map(self.df_interesting, 'price')
        self.df, self.df_interesting = Commons.append_tags_analysis_columns(self.df, self.df_interesting, "price")
        self.models_creation(self.df_interesting, 'price')

    def create_model(self, X_train, y_train, X_test, y_test):
        """
        De los mejores modelos que se encontraron para el dataset se elige entrenar un ExtraTreesRegressor

        :param X_train: La entrada de entrenamiento
        :param y_train: La salida esperada de entrenamiento
        :param X_test: La entrada de pruebas
        :param y_test: La salida esperada de pruebas
        """
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- Final Model evaluation ----------------------------\n" + pcolors.ENDC)
        temp_model = ExtraTreesRegressor(n_estimators=30, random_state=self.random_seed)
        temp_model.fit(X_train, y_train)
        dump(temp_model, './Resources/Persistence/pModel.joblib')
        cv_results = cross_val_score(temp_model, X_test, y_test, cv=5, n_jobs=-1)
        # output:
        min_score = round(min(cv_results), 4)
        max_score = round(max(cv_results), 4)
        mean_score = round(np.mean(cv_results), 4)
        std_dev = round(np.std(cv_results), 4)
        print(
            f'{"ETR"} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}')

    def models_creation(self, df, col):
        """
        Creación y evaluación de modelos

        Lo primero es seperar el dataset en tres:
            *   Conjunto de entrenamiento
            *   Conjunto de selección de hiper parametros
            *   Conjunto de prueba

        :param df: El dataframe que sobre el que se harán los modelos
        :param col: La columna de interes del dataframe
        """
        X = df.drop([col], axis=1)
        Y = df[col].astype(int)
        trainig_size = 0.6
        testing_size = 1 - trainig_size
        self.random_seed = 42
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=testing_size, random_state=self.random_seed,
                                                            shuffle=True)
        X_test, X_test_hp, y_test, y_test_hp = train_test_split(X, Y, test_size=0.5, random_state=self.random_seed,
                                                                shuffle=True)

        """En esta etapa escogemos algunos modelos que queremos entrenar y ver cómo se comportan con los datos de entrada.

        Escogemos una estrategía de validación cruzada para validar los resultados que obtenemos al entrenar los modelos.
        """

        base_models = [('DT_model', DecisionTreeClassifier(random_state=self.random_seed)),
                       ('RF_model', RandomForestClassifier(random_state=self.random_seed, n_jobs=-1)),
                       ('LR_model', LogisticRegression(random_state=self.random_seed, n_jobs=-1)),
                       ("XGB_model", XGBClassifier(random_state=self.random_seed, n_jobs=-1)),
                       ("ETR_model", ExtraTreesRegressor(n_estimators=20, random_state=self.random_seed))]
        # split data into 'kfolds' parts for cross validation,
        # use shuffle to ensure random distribution of data:
        self.kfolds = 5
        split = KFold(n_splits=self.kfolds, shuffle=True, random_state=self.random_seed)

        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- MODELS COMPARISON ----------------------------\n" + pcolors.ENDC)
        # Preprocessing, fitting, making predictions and scoring for every model:
        for name, model in base_models:
            temp_model = Pipeline(steps=[('model', model)])
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_train, y_train, cv=split, n_jobs=-1)
            # output:
            min_score = round(min(cv_results), 4)
            max_score = round(max(cv_results), 4)
            mean_score = round(np.mean(cv_results), 4)
            std_dev = round(np.std(cv_results), 4)
            print(
                pcolors.OKCYAN + pcolors.UNDERLINE + pcolors.BOLD + f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}' + pcolors.ENDC)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- MODELS COMPARISON ---------------------------->>>\n\n" + pcolors.ENDC)

        """# Ajuste de hiper parametros

    Seleccionamos los 3 modelos con mejores resultados y procedemos a ajustar algunos hiperparametros que nos permitan obtener mejores resultados.
    """

        # temp_model =
        # temp_model.fit(X_test_hp, y_tes_hp)
        # cv_results = cross_val_score(temp_model,X_train,y_train,cv=split,n_jobs=-1)
        # # output:
        # min_score = round(min(cv_results),4)
        # max_score = round(max(cv_results),4)
        # mean_score = round(np.mean(cv_results),4)
        # std_dev = round(np.std(cv_results),4)
        # print(f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}')

        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 30):
            temp_model = ExtraTreesRegressor(n_estimators=i, random_state=self.random_seed)
            temp_model.fit(X_train, X_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        # print(y_scores)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)

        self.create_model(X_train, y_train, X_test, y_test)

        """
        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 30):
            temp_model = DecisionTreeClassifier(random_state=self.random_seed, max_depth=i)
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- CORRELATION MATRIX ----------------------------\n" + pcolors.ENDC)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- CORRELATION MATRIX ---------------------------->>>\n\n" + pcolors.ENDC)

        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 20):
            temp_model = XGBClassifier(random_state=self.random_seed, n_jobs=-1, n_estimators=i)
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        # print(y_scores)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)

        temp_model = ExtraTreesRegressor(n_estimators=30, random_state=self.random_seed)
        temp_model.fit(X_train, y_train)
        cv_results = cross_val_score(temp_model, X_test, y_test, cv=split, n_jobs=-1)
        # output:
        min_score = round(min(cv_results), 4)
        max_score = round(max(cv_results), 4)
        mean_score = round(np.mean(cv_results), 4)
        std_dev = round(np.std(cv_results), 4)
        print(f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}')
        self.createModel(X_train,y_train,X_test,y_test)
        """
        """El modelo tuvo un resultado decente, que todavía puede mejorar, somos capaces de predecir el precio para un producto de manera adecuada casí un 60% de las veces, es una muestra de que vamos por buen camino y que podemos aportarle valor a wish."""
