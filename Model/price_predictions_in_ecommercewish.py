# -*- coding: utf-8 -*-
"""Price_Predictions_in_EcommerceWish.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RsVZXe6zZZSgjP4xr7WVrGcjoDp_zkMj

# Predicciones de precio en wish

En este nootebook se utiliza un dataset de las ventas de verano en el 2019 de Wish, la intención es ser capaces de predecir el precio de un producto basados en caracteristicas del producto que podamos observar o determinar, y qué estén incluidos en el dataset.

El primer paso es importar todas las librerias necesarias para el procesamiento de los datos e importar el dataset montado en una carpeta de drive.
"""


class pcolors:
    HEADER = '\033[95m'
    OKBLUE = '\033[94m'
    OKCYAN = '\033[96m'
    OKGREEN = '\033[92m'
    FAIL = '\033[91m'
    BOLD = '\033[1m'
    UNDERLINE = '\033[4m'
    ENDC = '\033[0m'


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import plotly.express as px
import plotly.graph_objects as go
from plotly.subplots import make_subplots
from sklearn.preprocessing import OneHotEncoder
from sklearn.model_selection import train_test_split

# models
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.linear_model import LogisticRegression
from xgboost.sklearn import XGBClassifier
from sklearn.ensemble import ExtraTreesRegressor
from sklearn.model_selection import KFold, cross_val_score
from sklearn.pipeline import Pipeline


# !pip install pywaffle --quiet
# from wordcloud import WordCloud

# from google.colab import drive

# drive.mount('/content/drive')

class PriceModel:

    def execute(self):
        # df = pd.read_csv("/content/drive/MyDrive/Proyecto Final IA/summer-products-with-rating-and-performance_2020-08.csv")
        df = pd.read_csv("./Resources/Datasets/summer-products.csv")
        # /summer-products-with-rating-and-performance_2020-08.csv
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- ORIGINAL ----------------------------\n" + pcolors.ENDC + pcolors.ENDC)
        print(df.columns)
        print("\n")
        print(df.shape)
        # https://www.kaggle.com/jmmvutu/summer-products-and-sales-in-ecommerce-wish
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- ORIGINAL ---------------------------->>>\n\n" + pcolors.ENDC)
        df_interesting = self.extract_interesting_features(df)
        print(df_interesting.shape)
        self.plot_missing_data(df_interesting)
        self.clean(df_interesting)
        """Para ver la calidad de las variables que escogimos decidimos ver el indice de correlación que tienen respecto al precio de venta."""
        self.print_correlation_map(df_interesting, 'price')
        self.plot_colors_vs_price(df_interesting)
        df_interesting = self.one_hot_encode(df_interesting)
        # correlation again
        self.print_correlation_map(df_interesting, 'price')
        df, df_interesting = self.append_tags_analysis_columns(df, df_interesting)
        self.models_creation(df_interesting, 'price')
        """# Identificar una lista de atributos a utilizar:
        
    Despues de realizar el EDA y de reflexionar acerca de que atributos podemos controlar realmente seleccionamos una lista de atributos con potencial de ser utilizados:

    title,title_orig,price,retail_price,currency_buyer,uses_ad_boosts,tags,product_color,product_variation_size_id,product_variation_inventory,
    shipping_option_name,shipping_option_price,shipping_is_express,countries_shipped_to,inventory_total,origin_country,merchant_title,merchant_name,
    merchant_info_subtitle,merchant_has_profile_picture,merchant_profile_picture,product_picture
    """

    def extract_interesting_features(self, df):
        interesting_features = ["product_color", "product_variation_inventory", "shipping_is_express", "origin_country",
                                "price", "uses_ad_boosts", "rating", "merchant_rating", "merchant_rating_count",
                                "merchant_has_profile_picture", "badge_product_quality", "has_urgency_banner"]
        interesting_features_asuming = ["units_sold", "product_color", "product_variation_inventory",
                                        "shipping_is_express", "origin_country", "price", "uses_ad_boosts", "rating",
                                        "merchant_rating", "merchant_rating_count", "merchant_has_profile_picture",
                                        "badge_product_quality", "has_urgency_banner", 'rating_count',
                                        'rating_five_count', 'rating_four_count', 'rating_three_count',
                                        'rating_two_count', 'rating_one_count']

        """Acerca de las varables que no podemos controlar, hay varias con se correlacionan bastante con la cantidad vendida, cómo lo son el rating_count y  los rating_x_count, pero dado que están por fuera de nuestro control, buscamos una manera de realacionaar esas variables con los tags."""

        df_interesting = df.loc[:, interesting_features]
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- INTERESTING FEATURES ----------------------------\n" + pcolors.ENDC)
        print(df_interesting.columns)
        print("\n")
        print(df_interesting.shape)
        print(df_interesting.info())
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- INTERESTING FEATURES ---------------------------->>>\n\n" + pcolors.ENDC)
        return df_interesting
        """Realizamos de nuevo la limpieza de los datos, para empezar el proceso de análitica.
    """

    def plot_missing_data(self, df):
        columns_with_null = df.columns[df.isna().sum() > 0]
        null_pct = (df[columns_with_null].isna().sum() / df.shape[0]).sort_values(ascending=False) * 100
        plt.figure(figsize=(8, 6));
        sns.barplot(y=null_pct.index, x=null_pct, orient='h')
        plt.title('% Na values in dataframe by columns')

        """Asignamos valores coherentes para remplazar los nans encontrados en el dataset."""
    def clean(self, df):
        nan_replace = {'has_urgency_banner': 0, 'origin_country': 'unknown', 'product_color': 'multicolor'}
        df.fillna(nan_replace, inplace=True)
        df = df.dropna()
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- INTERESTING FEATURES AFTER CLEANING ----------------------------\n" + pcolors.ENDC)
        print(df.info())
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- INTERESTING FEATURES AFTER CLEANING ---------------------------->>>\n\n" + pcolors.ENDC)
        # plt.figure(figsize=(20,12))
        # sns.heatmap(corr_map,annot=True,cmap='Blues')
        # plt.xticks(rotation=45,fontsize=14)
        # plt.yticks(rotation=45,fontsize=14)
        # plt.show()

        """# Ajuste de las variables:

    A partir de las variables que tenemos, intentaremos hacer ajustes para ayudar al modelamiento.

    Identifiquemos los colores que más venden y realizemos una clasificación en donde pongamos a los colores menos relevantes en una catégoria propia.
    """
    def plot_colors_vs_price(self, df):
        color_sale = df.groupby('product_color')['price'].mean()
        color_sale = color_sale.reset_index().sort_values(by='price', ascending=False)
        top_10 = color_sale.head(15)
        sns.barplot(data=top_10, x='product_color', y='price')

        # Ponemos los otros colores en other, para eliminar ruido
        top_10_vals = top_10.product_color.values
        df['product_color'][-df['product_color'].isin(top_10_vals)] = 'other'
        sns.barplot(data=df, x='product_color', y='price')

    def one_hot_encode(self, df):
        """Ahora realizaremos una codificación onehot, para convertir todas las variables categoricas en varias columnas de valores enteros binarios"""

        # print(df_interesting.columns)
        categories = df.loc[:, ['product_color', 'origin_country']]
        encoder = OneHotEncoder(sparse=False)
        categories_1hot = pd.DataFrame(encoder.fit_transform(categories))
        new_categories_nms = np.concatenate([encoder.categories_[0], encoder.categories_[1]])
        categories_1hot.columns = new_categories_nms
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- CATEGORIES ONE HOT ENCODING ----------------------------\n" + pcolors.ENDC)
        print(categories_1hot.info())
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- CATEGORIES ONE HOT ENCODING ---------------------------->>>\n\n" + pcolors.ENDC)
        df = pd.concat([df, categories_1hot], axis=1).drop(['product_color', 'origin_country'], axis=1)
        return df
        # print(df_interesting.info())

        """En esta parte agregaremos 2 columnas que se relacionan con los tags:


    *   best_tag_units_sold: Escogemos el valor del tag con las mejores ventas promedio
    *   Elemento de lista: Calculamos el promedio del promedio de las ventas de todos los tag

    El primer paso es crear un diccionario con todas los precios de los productos que tienen un tag especifico.
    """
    def append_tags_analysis_columns(self, df, df_interesting):
        dic_tags_uts = {}

        df_search = df.loc[:, ["tags", "price"]]
        # print(df_search)
        df_search = df_search.values
        for row in df_search:
            # print(row)
            tags = row[0].split(",")
            units = row[1]
            for tag in tags:
                if (tag in dic_tags_uts):
                    dic_tags_uts[tag].append(units)
                else:
                    dic_tags_uts[tag] = [units]
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- TAGS DICTIONARY ----------------------------\n" + pcolors.ENDC)
        print(dic_tags_uts)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- TAGS DICTIONARY ---------------------------->>>\n\n" + pcolors.ENDC)

        """Ahora creamos un diccionario que contiene el promedio de todos esos precios."""

        # Vamos a crear un nuevo diccionario donde solo se guarden los promedios

        for elem in dic_tags_uts:
            arr_tmp = np.array(dic_tags_uts[elem])
            dic_tags_uts[elem] = arr_tmp.mean()

        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- AVG PRICE TAGS DICTIONARY ----------------------------\n" + pcolors.ENDC)
        print(dic_tags_uts)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- AVG PRICE TAGS DICTIONARY ---------------------------->>>\n\n" + pcolors.ENDC)

        """Ahora creamos las listas que seran convertidas en columnas y seran agregadas al data set."""

        # Crear las listas que vamos a agregar al dataset

        grt_tag_price_list = []
        avrg_tag_price_list = []

        df_search = df.loc[:, ["tags", "units_sold"]]
        # print(df_search)
        df_search = df_search.values
        for row in df_search:
            # print(row)
            tags = row[0].split(",")
            best_tag = tags[0]
            best_tag_val = dic_tags_uts[best_tag]
            avrg_tag_val = 0

            for tag in tags:
                val = dic_tags_uts[tag]
                if (best_tag_val < val):
                    best_tag = tag
                    best_tag_val = val
                avrg_tag_val += val

            avrg_tag_val /= len(tags)
            grt_tag_price_list.append(best_tag_val)
            avrg_tag_price_list.append(avrg_tag_val)

        """Concatenamos las nuevas columnas al data set."""

        # Concatenar las nuevas columnas al data frame

        df_interesting["grt_tag_price"] = grt_tag_price_list
        df_interesting["avrg_tag_price"] = avrg_tag_price_list

        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- RESULTING DATA SET ----------------------------\n" + pcolors.ENDC)
        print(df_interesting.info())
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- RESULTING DATA SET ---------------------------->>>\n\n" + pcolors.ENDC)
        return df, df_interesting

        """# Creación de modelos

    Lo primero es seperar el dataset en tres:
    *   Conjunto de entrenamiento
    *   Conjunto de selección de hiper parametros
    *   Conjunto de prueba
    """
    def models_creation(self, df, col):
        X = df.drop([col], axis=1)
        Y = df[col].astype(int)
        trainig_size = 0.6
        testing_size = 1 - trainig_size
        random_seed = 42
        X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=testing_size, random_state=random_seed,
                                                            shuffle=True)
        X_test, X_test_hp, y_test, y_test_hp = train_test_split(X, Y, test_size=0.5, random_state=random_seed,
                                                                shuffle=True)

        """En esta etapa escogemos algunos modelos que queremos entrenar y ver cómo se comportan con los datos de entrada.

    Escogemos una estrategía de validación cruzada para validar los resultados que obtenemos al entrenar los modelos.
    """

        base_models = [('DT_model', DecisionTreeClassifier(random_state=random_seed)),
                       ('RF_model', RandomForestClassifier(random_state=random_seed, n_jobs=-1)),
                       ('LR_model', LogisticRegression(random_state=random_seed, n_jobs=-1)),
                       ("XGB_model", XGBClassifier(random_state=random_seed, n_jobs=-1)),
                       ("ETR_model", ExtraTreesRegressor(n_estimators=20, random_state=random_seed))]
        # split data into 'kfolds' parts for cross validation,
        # use shuffle to ensure random distribution of data:
        kfolds = 4
        split = KFold(n_splits=kfolds, shuffle=True, random_state=random_seed)

        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- MODELS COMPARISON ----------------------------\n" + pcolors.ENDC)
        # Preprocessing, fitting, making predictions and scoring for every model:
        for name, model in base_models:
            temp_model = Pipeline(steps=[('model', model)])
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_train, y_train, cv=split, n_jobs=-1)
            # output:
            min_score = round(min(cv_results), 4)
            max_score = round(max(cv_results), 4)
            mean_score = round(np.mean(cv_results), 4)
            std_dev = round(np.std(cv_results), 4)
            print(
                pcolors.OKCYAN + pcolors.UNDERLINE + pcolors.BOLD + f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}' + pcolors.ENDC)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- MODELS COMPARISON ---------------------------->>>\n\n" + pcolors.ENDC)

        """# Ajuste de hiper parametros

    Seleccionamos los 3 modelos con mejores resultados y procedemos a ajustar algunos hiperparametros que nos permitan obtener mejores resultados.
    """

        # temp_model =
        # temp_model.fit(X_test_hp, y_tes_hp)
        # cv_results = cross_val_score(temp_model,X_train,y_train,cv=split,n_jobs=-1)
        # # output:
        # min_score = round(min(cv_results),4)
        # max_score = round(max(cv_results),4)
        # mean_score = round(np.mean(cv_results),4)
        # std_dev = round(np.std(cv_results),4)
        # print(f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}')


        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 30):
            temp_model = ExtraTreesRegressor(n_estimators=i, random_state=random_seed)
            temp_model.fit(X_train, X_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        # print(y_scores)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)

        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 30):
            temp_model = DecisionTreeClassifier(random_state=random_seed, max_depth=i)
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- CORRELATION MATRIX ----------------------------\n" + pcolors.ENDC)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- CORRELATION MATRIX ---------------------------->>>\n\n" + pcolors.ENDC)

        x_vals = []
        y_scores = []
        # X_test.drop("prediction",axis=1)
        for i in range(1, 20):
            temp_model = XGBClassifier(random_state=random_seed, n_jobs=-1, n_estimators=i)
            temp_model.fit(X_train, y_train)
            cv_results = cross_val_score(temp_model, X_test_hp, y_test_hp, cv=split, n_jobs=-1)
            y_predict = cv_results.mean()
            x_vals.append(i)
            y_scores.append(y_predict)

        sns.scatterplot(x=x_vals, y=y_scores)
        # print(y_scores)
        print(X_test_hp.columns)
        print(temp_model.feature_importances_)

        temp_model = ExtraTreesRegressor(n_estimators=30, random_state=random_seed)
        temp_model.fit(X_train, y_train)
        cv_results = cross_val_score(temp_model, X_test, y_test, cv=split, n_jobs=-1)
        # output:
        min_score = round(min(cv_results), 4)
        max_score = round(max(cv_results), 4)
        mean_score = round(np.mean(cv_results), 4)
        std_dev = round(np.std(cv_results), 4)
        print(f'{name} cross validation accuracy score:{mean_score} +- {std_dev} (std) min:{min_score},max:{max_score}')

        """El modelo tuvo un resultado decente, que todavía puede mejorar, somos capaces de predecir el precio para un producto de manera adecuada casí un 60% de las veces, es una muestra de que vamos por buen camino y que podemos aportarle valor a wish."""

    def print_correlation_map(self, df, col):
        """Volvemos a revisar los indices de correlación ahora teniendo en cuenta las todas las nuevas variables creadas."""

        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "<<<---------------------------- CORRELATION MATRIX ----------------------------\n" + pcolors.ENDC)
        print(df.corr()[col].sort_values(ascending=False))
        print(
            pcolors.OKGREEN + pcolors.UNDERLINE + pcolors.BOLD + "   ---------------------------- CORRELATION MATRIX ---------------------------->>>\n\n" + pcolors.ENDC)
